# -*- coding: utf-8 -*-
"""211000DSLoanClassifierr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15RZqSCCt9ucQgpejqL0sLjwnpK0h0ik8
"""

!pip install xgboost scikit-learn matplotlib seaborn --quiet

# This code cell performs part of the loan classification task.
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split # Importing function to split dataset into training and test sets
from sklearn.preprocessing import LabelEncoder  # Importing LabelEncoder to convert categorical values to numeric
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Load Data
df = pd.read_csv("/content/loangrant - loangrant.csv")

# Drop duplicates
df.drop_duplicates(inplace=True)

# Handle missing values
df.replace("NA", np.nan, inplace=True)
df.replace("n/a", np.nan, inplace=True)
df.replace("N/A", np.nan, inplace=True)

# Drop Loan ID & Customer ID
df.drop(['Loan ID', 'Customer ID'], axis=1, inplace=True)

# Remove rows with all NaN values
df.dropna(how='all', inplace=True)

# Clean dollar and percent signs if any
df['Monthly Debt'] = df['Monthly Debt'].replace('[\$,]', '', regex=True).astype(float)
df['Annual Income'] = pd.to_numeric(df['Annual Income'], errors='coerce')

# Impute numeric columns
num_cols = df.select_dtypes(include=[np.number]).columns
imputer = SimpleImputer(strategy='median')
df[num_cols] = imputer.fit_transform(df[num_cols])

# Label encode categorical variables
cat_cols = df.select_dtypes(include=['object']).columns
for col in cat_cols:
    df[col] = df[col].astype(str)
    df[col] = LabelEncoder().fit_transform(df[col])

# Features and Target
X = df.drop("Loan Status", axis=1)
y = df["Loan Status"]

# Encode target
y = LabelEncoder().fit_transform(y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.metrics import confusion_matrix, accuracy_score, matthews_corrcoef, roc_auc_score, roc_curve
# Import evaluation metrics: confusion matrix, accuracy, Matthews correlation coefficient, AUC score, and ROC curve

def evaluate_model(model, X_test, y_test, model_name):
    # Define a function to evaluate a classification model using various performance metrics

    from sklearn.metrics import recall_score, precision_score
    # Import recall and precision metrics locally within the function

    y_pred = model.predict(X_test)
    # Predict the target values for the test set

    cm = confusion_matrix(y_test, y_pred)
    # Compute the confusion matrix

    tn, fp, fn, tp = cm.ravel()
    # Extract True Negatives, False Positives, False Negatives, and True Positives from the confusion matrix

    acc = accuracy_score(y_test, y_pred)
    # Calculate the overall accuracy of the model

    sensitivity = recall_score(y_test, y_pred)
    # Calculate sensitivity (recall), i.e., TP / (TP + FN)

    specificity = tn / (tn + fp)
    # Calculate specificity, i.e., TN / (TN + FP)

    mcc = matthews_corrcoef(y_test, y_pred)
    # Calculate the Matthews Correlation Coefficient (balanced measure even if classes are imbalanced)

    y_prob = model.predict_proba(X_test)[:,1] if hasattr(model, "predict_proba") else model.decision_function(X_test)
    # Get the predicted probability scores for the positive class (1), depending on model's method

    auc = roc_auc_score(y_test, y_prob)
    # Calculate the Area Under the ROC Curve (AUC)

    print(f"=== {model_name} ===")
    # Print model name

    print(f"Confusion Matrix:\n{cm}")
    # Print the confusion matrix

    print(f"Accuracy: {acc:.4f}")
    # Print accuracy score rounded to 4 decimal places

    print(f"Sensitivity (Recall): {sensitivity:.4f}")
    # Print recall score (sensitivity)

    print(f"Specificity: {specificity:.4f}")
    # Print specificity score

    print(f"MCC: {mcc:.4f}")
    # Print Matthews Correlation Coefficient

    print(f"AUC: {auc:.4f}")
    # Print the AUC score

    fpr, tpr, _ = roc_curve(y_test, y_prob)
    # Compute false positive rate and true positive rate for the ROC curve

    plt.figure(figsize=(6,4))
    # Set the size of the ROC curve plot

    plt.plot(fpr, tpr, label=f"{model_name} (AUC = {auc:.2f})")
    # Plot the ROC curve with AUC score in the legend

    plt.plot([0,1],[0,1],'--',color='gray')
    # Plot the diagonal line (representing random guessing)

    plt.xlabel("False Positive Rate")
    # Label the x-axis

    plt.ylabel("True Positive Rate")
    # Label the y-axis

    plt.title("ROC Curve")
    # Set the title of the plot

    plt.legend()
    # Show the legend

    plt.grid()
    # Show gridlines for better readability

    plt.show()
    # Display the plot

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
evaluate_model(dt, X_test, y_test, "Decision Tree")

from sklearn.ensemble import BaggingClassifier
bag = BaggingClassifier()
bag.fit(X_train, y_train)
evaluate_model(bag, X_test, y_test, "Bagging")

from xgboost import XGBClassifier
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb.fit(X_train, y_train)
evaluate_model(xgb, X_test, y_test, "XGBoost")

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
evaluate_model(rf, X_test, y_test, "Random Forest")

from sklearn.svm import SVC
svm = SVC(probability=True)
svm.fit(X_train, y_train)
evaluate_model(svm, X_test, y_test, "Support Vector Machine")

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
evaluate_model(knn, X_test, y_test, "K-Nearest Neighbors")